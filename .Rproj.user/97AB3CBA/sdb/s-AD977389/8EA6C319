{
    "contents" : "---\ntitle: 'Exercising Right: Using sensors to ensure proper form'\noutput: html_document\n---\n### michael downs\n\n## synopsis\nMany people measure the amount they exercise (miles run, weights lifted, etc.). Few measure the quality of their exercise, their \"form\". Using data from sensors placed on the belts, forearms, arms, and dumbells of six participants, I used four algorithms to classify barbell lifts. Classifications range from correct form (class A) to four incorrect forms including throwing the elbows (class B), throwing the hips (class E), etc..\n\nI found random forest (rf) and boosting (gbm) to be effective at classifying activites based on sensor output. Both algorithms achieved 97%+ sensitivity and 99%+ specificity scores on cross validation. (The random forest model scored 20/20 on the project test set.) Further, both models maintained high cross validation scores: 1. across four train/test datasets that were constructed using different methodologies and 2. across a range of variables from over 100 down to as small as 10. K-nearest neighbors (knn) and basic regression trees (rpart) were less successful with sensitivity scores of $\\approx$ 88% and $\\approx$ 53% on cross validation, respectively. \n\nI walk thru model construction below including use of cross validation, estimates of out-of-sample error rates and the rationale for decisions made along the way.\n\n## the model\n### getting data\nThe training data set included 160 variables and about 20,000 records. Each record contained a subset of sensor readings sampled during the exercise period for each of the six particpants. Basic exploratory analysis showed that 67% of the columns contained 97% NA values which would have to be cleaned. That analysis also identified a roughly even distribution of observations among the three classes and signficant signal that could be seen when fields were plotted against each other (below).\n\n```{r eval=TRUE,cache=TRUE,echo=FALSE,results='hide',warning=FALSE,fig.show='asis',fig.height=4,fig.width=7}\n\ntrain=read.csv(\"pml-training.csv\",header=TRUE,sep=\",\",quote=\"\\\"\",dec=\".\",fill=TRUE,comment.char=\"\")\ntest=read.csv(\"pml-testing.csv\",header=TRUE,sep=\",\",quote=\"\\\"\",dec=\".\",fill=TRUE,comment.char=\"\")\n\ndim(train);dim(test);unique(train$classe) # X index. numerics as factors. many NA's. some div/0's. \ntrain=train[,2:160];test=test[,2:160] # get rid of index\n#159th column \"classe\"\n\npar(mfrow=c(1,2))\nplot(unique(train$classe),ylim=c(0,1000),main=\"class distribution\")\nfor(i in unique(train$user_name)){\n     temp=train[train$user_name==i,]\n     classes=table(temp$classe)\n     lines(unique(train$classe),classes)\n     if(exists(\"class.means\")){class.means=rbind(class.means,classes)}else{class.means=classes}\n}\nlines(unique(train$classe),colMeans(class.means),col=\"red\",lwd=5)\nrm(class.means,temp,classes) # roughly equal distribution among the classes by individual\n\nplot(train$roll_belt,train$yaw_belt,col=train$classe,main=\"clearly signal here\") # hello\n\nnum.na=NULL; for(i in 1:dim(train)[2]){num.na[i]=sum(is.na(train[,i]))};num.na\n# can't impute all NAs. need to try something else.\n```\n\n### cleaning data\n\nI generated four train/test datasets. While I deleted timestamp and window columns for all datasets (to the extent exercisers are following a script, these columns provide information that wouldn't be available otherwise), each dataset was developed according to its own methodology. Highlights include:\n1. $\\emph{train.base:} I replaced NA's by the means for their respective columns. Given most algorithms use column variance for predictions, this approach elimnates NAs while not changing column variance.\n2. $\\emph{train.nzv:} I deleted rows with near zero varaiance (nzv()) then set NAs in remaining columns to the respective column mean.\n3. $\\emph{train.na:} I deleted columns with greater than 97% NA. \n4. $\\emph{train.rev:} I converted factor to numeric using $levels(train.rev[,i]))[train.rev[,i]]$, then set any remaining NAs to the respective column means. \n\nThe effect of these changes and others not covered is illusrated in the kurtosis graphic below. From left to right I show how how the data was imported, what the data looke like using for train.base and what the data looked like for train.rev. \n\n```{r eval=TRUE,cache=TRUE,echo=FALSE,results='hide',warning=FALSE,fig.show='asis',fig.height=4,fig.width=7}\nlibrary(RANN);library(caret)\n\n## train.base is baseline. zoom in on a problem field\ntrain.base=train[,c(1,7:159)] # remove non-movement columns\npar(mfrow=c(1,3));plot(train.base$kurtosis_roll_belt,col=train.base$classe,main=\"kurtosis roll - train\")\n\n# convert factor to numerics\nfor(i in 2:dim(train.base)[2]-1){\n     if(is.factor(train.base[,i])){train.base[,i]=(as.numeric(train.base[,i])-1)}\n}\n\n# train.na deletes columns w/ over 97% NA\nnum.na=NULL\nfor(i in 1:dim(train.base)[2]){num.na[i]=sum(is.na(train.base[,i]))>(0.97*dim(train.base)[1])};num.na\ntrain.na=train.base[,!num.na]\n\n# train.nzv removes near zero variables. \nnzv=nearZeroVar(train.base,saveMetrics=TRUE)\ntrain.nzv=train.base[,!names(train.base)%in%rownames(nzv[nzv[,4]==TRUE,])]\n\n## nzv deleted more than na. 60% overlap remains\nsum(nzv[,4]);sum(num.na)\nlength(intersect(names(train.nzv),names(train.na)))\ndim(train.base);dim(train.nzv);dim(train.na)\n\n# converting remaining NA fields to respective column means\nfor(i in 2:(dim(train.base)[2]-1)){train.base[is.na(train.base[,i]),i]=mean(train.base[,i],na.rm=TRUE)}\nfor(i in 2:(dim(train.nzv)[2]-1)){train.nzv[is.na(train.nzv[,i]),i]=mean(train.nzv[,i],na.rm=TRUE)}\nfor(i in 2:(dim(train.na)[2]-1)){train.na[is.na(train.na[,i]),i]=mean(train.na[,i],na.rm=TRUE)}\n\nplot(train.base$kurtosis_roll_belt,col=train$classe,main=\"kurtosis roll - base\")\nsum(is.na(train.base));sum(is.na(train.nzv));sum(is.na(train.na))\n\n# careful cleaning of train.rev\ntrain.rev=train[,c(1,7:159)] # remove non-movement columns\n\n## 67 columns w/ over 97% NA's. converting factor vars using: levels(train.rev[,i]))[train.rev[,i]]\nnum.na=NULL;for(i in 1:dim(train.rev)[2]){num.na[i]=sum(is.na(train.rev[,i]))};num.na\nfor(i in 2:(dim(train.rev)[2]-1)){\n     if(is.factor(train.rev[,i])){train.rev[,i]=\n                                       as.numeric(levels(train.rev[,i]))[train.rev[,i]]}\n     train.rev[is.na(train.rev[,i]),i]=mean(train.rev[,i],na.rm=TRUE)\n}\nplot(train.rev$kurtosis_roll_belt,col=train.rev$classe,main=\"kurtosis roll - rev\")\n\n## coversion above introduced 6 columns w/ NaN's - deleting \nnum.na=NULL;for(i in 1:dim(train.rev)[2]){num.na[i]=sum(is.na(train.rev[,i]))};num.na\nnum.na=NULL;for(i in 1:dim(train.rev)[2]){num.na[i]=sum(is.na(train.rev[,i]))>0};num.na\ntrain.rev=train.rev[,!num.na]\nnum.na=NULL;for(i in 1:dim(train.rev)[2]){num.na[i]=sum(is.na(train.rev[,i]))};num.na\n\n## removing three columns were problematic during PCA\nwhich(names(train.rev)==\"amplitude_yaw_forearm\")\nwhich(names(train.rev)==\"amplitude_yaw_dumbbell\")\nwhich(names(train.rev)==\"amplitude_yaw_belt\")\ntrain.rev=train.rev[,-c(18,91,127)]\n\n# note: didn't find boxcox to be additive to tree models. \n```\n\n### pca pre-processing\nI performed feature selection in two steps. The first was pca analysis. I based my approach on the lecture where pca was used only for highly correlated variables (rather than to reduce dimensions and regularize all variables). Specifically, I looped over the data sets five times replacing highly correlated variables with their principal components. As the approach was a bit involved, I've included the code below. This approach reduced variables between 25% and 30% across data sets. \n\n```{r eval=TRUE,cache=TRUE,echo=TRUE,results='hide',warning=FALSE,fig.show='asis',fig.height=4,fig.width=7}\ndset=list(ds=list(\"train.base\",\"train.nzv\",\"train.na\",\"train.rev\"),\n          trnx=list(train.base,train.nzv,train.na,train.rev),\n          trny=train$classe)\n\npar(mfrow=c(1,1))\nplot(1:5,type=\"n\",main=\"pca dimension reductions\",ylim=c(0,160),xlab=\"pca iterations\",ylab=\"total variables\")\n\nfor(k in 1:length(dset$trnx)){\n     target=dset$trnx[[k]][,c(2:(dim(dset$trnx[[k]])[2]-1))]\n     gph.line=NULL\n     cat(\"********** new file: \",dset$ds[[k]],\"\\n\")\n     for(i in 1:5){ # was 7\n          cat(\"1. start: dim(target)\",dim(target),\"\\n\")\n          gph.line[i]=dim(target)[2]\n          cor.decs=matrix(NA,dim(target)[2],2)\n          M=abs(cor(target)) # find predictors w/ high correlation\n          diag(M)=0 # set cor for vars w/ themselves to 0 \n     \n          for(j in 1:dim(target)[2]){\n               cor.decs[j,1]=j\n               cor.decs[j,2]=length(which(M[,j]>max(0.5,(1-0.1*i)),arr.ind=T))\n          }\n          clean=target[,c(which(M[,which.max(cor.decs[,2])]>max(0.5,(1-0.1*i)),arr.ind=T),\n                          which.max(cor.decs[,2]))]\n     \n          cat(\"2. consolidating: \",cor.decs[which.max(cor.decs[,2]),][2]+1,\"\\n\")\n          comps=prcomp(clean);print(summary(comps))\n     \n          target=target[,-c(which(M[,which.max(cor.decs[,2])]>max(0.5,(1-0.1*i)),arr.ind=T),\n                            which.max(cor.decs[,2]))]\n          \n          tmp=as.matrix(summary(comps)$importance);pc.cut=which(tmp[3,]>0.95)[1]\n          comps=comps$x[,1:pc.cut];comps=as.data.frame(comps)\n          for(z in 1:pc.cut){\n               colnames(comps)[z]=c(sprintf(\"%s_%s_%s_%s\",\"iter\",i,\"pc\",z))}\n          cat(\"3. into: \",dim(comps)[2],\"\\n\")\n          \n          target=cbind(target,comps)\n     }\n     dset$trnx[[k]]=target\n     lines(1:5,gph.line,col=k,lwd=2)\n}\nlegend(\"bottomleft\",c(\"train.base  \",\"train.nzv  \",\"train.na  \",\"train.rev  \"),col=c(1,2,3,4),lwd=2)\n\nrm(tmp,clean,target)\nfor(i in 1:4){dset$trnx[[i]]$user_name=train$user_name\n              dset$trnx[[i]]$classe=train$classe}\n```\n\n### models\nI built a two loop model training structure to iterate thru the datasets outlined above and a set of core algorithms including regression trees (rpart), k-nearest neighbors (knn), random forest (rf) and boosting (gbm). Each iteration of the internal loop fit the model, made predictions on a cross validation set and stored off the fit for subsequent processing. \n\nGetting thru the dataset required careful resource management. While data cleaning / pca eliminated $\\frac{1}{3}$ to $\\frac{1}{2}$ of the columns, early iterations from 70 to 115 columns. Accordingly, I trimmed train/test database by up to 50%. I also enabled parallel processing.\n\nBeyond cross vaidation performed within the Caret function which showed 95-99% accuracy (1%-5% OOB error rates), I separately $\\textbf{cross validated}$ each dataset / algorithm combination using a 70 / 30 train / holdout test set cross validation. \n\nAs the approach was fairly involved, I've provided code below. Below that are the cross validated prediction sensitivity and specificity for each model. The left graphic shows all the models inclding the outperformance of the rf and gbm models. The right looks more closely at the rf and gbm results. \n\n```{r eval=FALSE,cache=TRUE,echo=TRUE,results='hold',warning=FALSE,fig.show='hide',fig.height=4,fig.width=7}\n\n# controller for algo iterations\nalgos=list(algo=list(\"rpart\",\"rf\",\"knn\",\"gbm\")) ## logistic, lda failed.\n\n# structures to hold rf fits\nfset=list(data=list(\"train.base\",\"train.nzv\",\"train.na\",\"train.rev\"),\n          rpart=list(NA,NA,NA,NA),\n          rf=list(NA,NA,NA,NA),\n          knn=list(NA,NA,NA,NA),\n          gbm=list(NA,NA,NA,NA))\n\n# process data sets, algos, folds\n# rm(res,res.mstr)\n\n## enable parallel processing\nlibrary(parallel); library(doParallel)\nregisterDoParallel(clust <- makeForkCluster(detectCores()))\n\nfor(i in 1:length(dset$ds)){\n     # cut data in half\n     train.cv=dset$trnx[[i]][train$user_name==unique(train$user_name)[1] | \n                                  train$user_name==unique(train$user_name)[2] | \n                                  train$user_name==unique(train$user_name)[3],]\n     \n     for(k in 1:length(algos$algo)){\n               # train and test sets\n               set.seed(123)\n               inTrain=createDataPartition(y=train.cv$classe,p=0.7,list=FALSE)\n               training=train.cv[inTrain,]\n               testing=train.cv[-inTrain,]\n               \n               cat(\"******* FITTING: algo:\",algos$algo[[k]],\", data dim: \",dim(training),\"\\n\")\n               # fit and predict\n               fit=train(classe~.,training,method=algos$algo[[k]])\n               \n               # store and print fit\n               fset[[k+1]][[i]]=fit;print(fit)\n               \n               # predict\n               preds=predict(fit,testing)\n               \n               # create output table\n               res=as.data.frame(t(as.matrix(c(algos$algo[[k]],dset$ds[[i]]))))\n               colnames(res)=c(\"algo\",\"data set\")\n               sense=round(t(confusionMatrix(preds,testing$classe)$byClass[,1]),3)\n               spec=round(t(confusionMatrix(preds,testing$classe)$byClass[,2]),3)\n               res=cbind(res,sense,round(mean(sense),3),spec,round(mean(spec),3))\n               if(exists(\"res.mstr\")){res.mstr=rbind(res.mstr,res)}else{res.mstr=res}\n               print(res.mstr)\n     }\n}\nstopCluster(clust)\n```\n\n```{r eval=TRUE,cache=TRUE,echo=FALSE,results='hide',warning=FALSE,fig.show='asis',fig.height=4,fig.width=7}\n# was cache=FALSE,eval=TRUE,echo=FALSE,results='markup'   \npar(mfrow=c(1,2))\nplot(1-res.mstr[,14],res.mstr[,8],xlab=\"mean(1-specificity (fpr))\",ylab=\"mean(sensitivity (tpr))\",xlim=c(0,0.3),ylim=c(0.43,1),main=\"'ROC' - all algos\",pch=19,cex=1.5,col=c(1:(5*4)))\ntext(x=1-res.mstr[,14],y=res.mstr[,8],\n     labels=sprintf(\"%s\",res.mstr[,1]),\n     pos=4,cex=0.75,col=\"blue\",offset=1)\n\nplot(1-res.mstr[1:16,14],res.mstr[1:16,8],xlab=\"mean(1-specificity (fpr))\",ylab=\"mean(sensitivity (tpr))\",xlim=c(0,0.11),ylim=c(0.945,1),main=\"'ROC' - rf/gbm\",pch=19,cex=1.5,col=c(1:(5*4)))\ntext(x=1-res.mstr[,14],y=res.mstr[,8],\n     labels=sprintf(\"%s-%s\",res.mstr[,1],res.mstr[,2]),\n     pos=4,cex=0.75,col=\"blue\",offset=1)\n\ndev.copy(pdf,file=\"init_mod_select_2.pdf\");dev.off()\n\n```\n\n### top 20 vars\nFinally, I re-ran the rf and gbm algorithms using the train.rev database and just their top variables. I compared four sets of results for different numbers of top variables including 100, 20, 15 and 10. Interestingly the performance did not degrade significantly for either algorithm. While rf moved from 99.5% to 97.5% accuracy between the 100 variable and 10 variable models, gbm actually increased its accuracy from 97% to 98% using as variables decreased from 100 to 15. \n\n```{r eval=FALSE,cache=TRUE,echo=FALSE,results='hide',warning=FALSE,fig.show='hide',fig.height=4,fig.width=7}\n# cache=TRUE,eval=FALSE,echo=FALSE  \n# rf\n## build new dataset \ntrain.cv=dset$trnx[[4]][train$user_name==unique(train$user_name)[4] | \n                             train$user_name==unique(train$user_name)[5] | \n                             train$user_name==unique(train$user_name)[6],]\n\nimp.vars=as.data.frame(varImp(fset[[3]][[4]])[1]);imp.vars$row.nms=rownames(imp.vars)\nimp.vars=imp.vars[order(imp.vars[,1],decreasing=TRUE),][1:5,]\ntrain.rf=train.cv[,names(train.cv) %in% imp.vars[,2]];train.rf$classe=train.cv$classe\n\nset.seed(9)\ninTrain=createDataPartition(y=train.rf$classe,p=0.7,list=FALSE)\ntraining=train.rf[inTrain,]\ntesting=train.rf[-inTrain,]\n\n## fit model\nlibrary(parallel); library(doParallel)\nregisterDoParallel(clust <- makeForkCluster(detectCores()))\nfit.rf=train(classe~.,training,\"rf\")\nprint(fit.rf)\nstopCluster(clust)\n\n# predict and tabulate\npreds=predict(fit.rf,testing)   \nres=as.data.frame(t(as.matrix(c(\"rf\",\"train.rev(5)\"))))\ncolnames(res)=c(\"algo\",\"data set\")\nsense=round(t(confusionMatrix(preds,testing$classe)$byClass[,1]),3)\nspec=round(t(confusionMatrix(preds,testing$classe)$byClass[,2]),3)\nres=cbind(res,sense,round(mean(sense),3),spec,round(mean(spec),3))\nif(exists(\"res.mstr\")){res.mstr=rbind(res.mstr,res)}else{res.mstr=res}\nprint(res.mstr)\n\n# gbm\nimp.vars=as.data.frame(varImp(fset[[5]][[4]])[1]);imp.vars$row.nms=rownames(imp.vars)\nimp.vars=imp.vars[order(imp.vars[,1],decreasing=TRUE),][1:10,]\ntrain.gbm=train.cv[,names(train.cv) %in% imp.vars[,2]];train.gbm$classe=train.cv$classe\n\nset.seed(9)\ninTrain=createDataPartition(y=train.gbm$classe,p=0.7,list=FALSE)\ntraining=train.gbm[inTrain,]\ntesting=train.gbm[-inTrain,]\n\n## fit model\nlibrary(parallel); library(doParallel)\nregisterDoParallel(clust <- makeForkCluster(detectCores()))\nfit.gbm=train(classe~.,training,\"gbm\")\nprint(fit.gbm)\nstopCluster(clust)\n\n# predict and tabulate\npreds=predict(fit.gbm,testing)   \nres=as.data.frame(t(as.matrix(c(\"gbm\",\"train.rev(10)\"))))\ncolnames(res)=c(\"algo\",\"data set\")\nsense=round(t(confusionMatrix(preds,testing$classe)$byClass[,1]),3)\nspec=round(t(confusionMatrix(preds,testing$classe)$byClass[,2]),3)\nres=cbind(res,sense,round(mean(sense),3),spec,round(mean(spec),3))\nif(exists(\"res.mstr\")){res.mstr=rbind(res.mstr,res)}else{res.mstr=res}\nprint(res.mstr)\n```\n\n```{r eval=TRUE,cache=TRUE,echo=FALSE,results='hide',warning=FALSE,fig.show='asis',fig.height=4,fig.width=7}\n# cache=FALSE,eval=TRUE,echo=FALSE,results='markup'  \n# output chart\npar(mfrow=c(1,1))\npr.mst=res.mstr[c(14,16,17,18,19,22,23,24),c(1,2,8,14)]\nplot(1-pr.mst[,4],pr.mst[,3],xlab=\"mean(1-specificity (fpr))\",ylab=\"mean(sensitivity (tpr))\",\n     xlim=c(0,0.015),ylim=c(0.96,1),main=\"'ROC' - rf/gbm\",pch=19,cex=1.5,col=c(2:9))\ntext(1-pr.mst[,4],pr.mst[,3],labels=sprintf(\"%s-%s\",pr.mst[,1],pr.mst[,2]),pos=4,\n     cex=0.75,col=\"blue\",offset=1)\n```\n\n```{r cache=FALSE,eval=FALSE,echo=FALSE}\n\nlibrary(parallel); library(doParallel)\nregisterDoParallel(clust <- makeForkCluster(detectCores()))\n\n     set.seed(123)\n     inTrain=createDataPartition(y=train.rev.imp.20$classe,p=0.7,list=FALSE)\n     training=train.rev.imp.20[inTrain,]\n     testing=train.rev.imp.20[-inTrain,]\n     fit.imp.20=train(classe~.,training,method=\"rf\",prox=TRUE)\n     \n     pred=predict(fit.imp.20,testing)\n     confusionMatrix(pred,testing$classe)\n     varImp(fit.imp.20);plot(varImp(fit.imp.20));fit.imp.20$finalModel$importance\n\nstopCluster(clust)\n\nanswers=as.character(predict(fit.imp.20,test))\n\npml_write_files = function(x){\n  n = length(x)\n  for(i in 1:n){\n    filename = paste0(\"problem_id_\",i,\".txt\")\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n  }\n}\n \npml_write_files(answers)\n\n``` \n",
    "created" : 1431649948858.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2432994996",
    "id" : "8EA6C319",
    "lastKnownWriteTime" : 1427064264,
    "path" : "~/Documents/Pers/Ed/Courses/JH7 - machine learning/JH7 - code/jh7-project.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}